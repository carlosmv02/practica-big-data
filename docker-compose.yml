version: '3.8'

services:
  # MongoDB - Base de datos
  mongodb:
    image: mongo:latest
    container_name: practica-mongodb
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db
    networks:
      - prediction-network
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Kafka - Sistema de mensajer√≠a
  kafka:
    image: bitnami/kafka:latest
    container_name: practica-kafka
    ports:
      - "9092:9092"
    environment:
      # KRaft mode configuration
      KAFKA_CFG_NODE_ID: 0
      KAFKA_CFG_PROCESS_ROLES: controller,broker
      KAFKA_CFG_CONTROLLER_QUORUM_VOTERS: 0@kafka:9093
      KAFKA_CFG_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9093
      KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_CFG_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CFG_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      # Topic auto-creation
      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: "true"
      # Log settings
      KAFKA_CFG_LOG_DIRS: /bitnami/kafka/data
    volumes:
      - kafka_data:/bitnami/kafka
    networks:
      - prediction-network
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --bootstrap-server localhost:9092 --list"]
      interval: 10s
      timeout: 10s
      retries: 5

  # Spark Master - Coordinador de Spark
  spark-master:
    image: bitnami/spark:3.5.3
    container_name: practica-spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8080:8080"
      - "7077:7077"
    networks:
      - prediction-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Spark Worker 1 - Worker de procesamiento
  spark-worker-1:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: practica-spark-worker-1
    depends_on:
      spark-master:
        condition: service_healthy
      kafka:
        condition: service_healthy
      mongodb:
        condition: service_healthy
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    networks:
      - prediction-network

  # Spark Worker 2 - Worker de procesamiento (redundancia)
  spark-worker-2:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: practica-spark-worker-2
    depends_on:
      spark-master:
        condition: service_healthy
      kafka:
        condition: service_healthy
      mongodb:
        condition: service_healthy
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    networks:
      - prediction-network

  # Flask Web Application
  flask-app:
    build:
      context: .
      dockerfile: Dockerfile.flask
    container_name: practica-flask
    depends_on:
      mongodb:
        condition: service_healthy
      kafka:
        condition: service_healthy
    ports:
      - "5001:5001"
    environment:
      - FLASK_ENV=development
      - PYTHONUNBUFFERED=1
    networks:
      - prediction-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  prediction-network:
    driver: bridge

volumes:
  mongodb_data:
  kafka_data:
